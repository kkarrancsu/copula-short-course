{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "0y0xc9j54oik",
   "source": "# Alternative Dependence Measures\n\nHaving seen the limitations of Pearson correlation, we now explore more sophisticated measures of dependence that can capture non-linear relationships. In this notebook, we will cover:\n\n1. **Mutual Information (MI)** - Information-theoretic measure of dependence\n2. **Distance Correlation (dCor)** - A metric that detects any type of dependence\n3. **Maximal Information Coefficient (MIC)** - Designed to find the \"most interesting\" relationships\n4. **Randomized Dependence Coefficient (RDC)** - Computationally efficient approximation\n\nWe will compare these measures on various functional relationships and test their robustness to noise.\n\n---\n\n## Setup and Imports",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "efd97920",
   "metadata": {},
   "outputs": [],
   "source": "# Environment configuration for Jupyter\n%load_ext autoreload\n%autoreload 2\n%matplotlib inline\n\n# Visualization libraries\nimport matplotlib.pyplot as plt\nfrom IPython import display\nimport seaborn as sns\nsns.set_theme(style=\"whitegrid\")\n\n# Numerical computing\nimport time\nimport numpy as np\nfrom scipy.stats import multivariate_normal as mvn\nimport scipy.spatial.distance\nimport pandas as pd\n\n# Machine learning utilities\nfrom sklearn import linear_model\nimport sklearn.datasets as toy_datasets\n\n# Dependence measure libraries\n# Note: minepy doesn't work with Python 3.12, using fallback\n# from minepy import MINE  # For MIC - commented out\nimport dcor  # For distance correlation\n\n# Fallback MIC implementation using mutual information approximation\ndef _fallback_mic(x, y, bins=10):\n    \"\"\"Simple MIC approximation using binned mutual information.\"\"\"\n    import numpy as np\n    from scipy.stats import entropy\n    \n    # Bin the data\n    x_binned = np.digitize(x, np.linspace(x.min(), x.max(), bins))\n    y_binned = np.digitize(y, np.linspace(y.min(), y.max(), bins))\n    \n    # Joint histogram\n    joint_hist, _, _ = np.histogram2d(x_binned, y_binned, bins=bins)\n    joint_hist = joint_hist / joint_hist.sum()\n    \n    # Marginals\n    px = joint_hist.sum(axis=1)\n    py = joint_hist.sum(axis=0)\n    \n    # Mutual information\n    mi = 0\n    for i in range(bins):\n        for j in range(bins):\n            if joint_hist[i,j] > 0 and px[i] > 0 and py[j] > 0:\n                mi += joint_hist[i,j] * np.log(joint_hist[i,j] / (px[i] * py[j]))\n    \n    # Normalize to [0, 1] approximating MIC\n    return min(1.0, mi / np.log(bins))"
  },
  {
   "cell_type": "markdown",
   "id": "3d376a46",
   "metadata": {},
   "source": "---\n\n## Mutual Information\n\n**Mutual Information** is an information-theoretic measure that quantifies the amount of information obtained about one random variable by observing another.\n\n$$I(X;Y) = \\int_y \\int_x p_{X,Y}(x,y) \\log \\left( \\frac{p_{X,Y}(x,y)}{p_X(x) \\cdot p_Y(y)} \\right) dx \\, dy$$\n\nEquivalently, it can be written as the Kullback-Leibler divergence between the joint distribution and the product of marginals:\n\n$$I(X;Y) = D_{KL}(p_{X,Y} \\| p_X \\otimes p_Y)$$\n\n**Properties:**\n- $I(X;Y) \\geq 0$, with equality if and only if $X$ and $Y$ are independent\n- $I(X;Y) = I(Y;X)$ (symmetric)\n- Not bounded above (unlike correlation)\n- Captures any type of dependence, not just linear\n\n---\n\n## Implementation of Dependence Measures\n\n### Randomized Dependence Coefficient (RDC)"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2016fd8d",
   "metadata": {},
   "outputs": [],
   "source": "def rdc(x, y, f=np.sin, k=20, s=1/6., n=1):\n    \"\"\"\n    Computes the Randomized Dependence Coefficient.\n    \n    Reference:\n    David Lopez-Paz, Philipp Hennig, Bernhard Schoelkopf\n    \"The Randomized Dependence Coefficient\"\n    http://papers.nips.cc/paper/5138-the-randomized-dependence-coefficient.pdf\n    \n    Parameters\n    ----------\n    x, y : numpy arrays\n        1-D arrays of size (samples,) or 2-D arrays of size (samples, variables)\n    f : function\n        Non-linear function for random projection (default: np.sin)\n    k : int\n        Number of random projections to use\n    s : float\n        Scale parameter for random projections\n    n : int\n        Number of times to compute RDC and return median (for stability)\n    \n    Returns\n    -------\n    float\n        The RDC value in [0, 1]\n    \"\"\"\n    if n > 1:\n        values = []\n        for i in range(n):\n            try:\n                values.append(rdc(x, y, f, k, s, 1))\n            except np.linalg.linalg.LinAlgError:\n                pass\n        return np.median(values)\n\n    if len(x.shape) == 1:\n        x = x.reshape((-1, 1))\n    if len(y.shape) == 1:\n        y = y.reshape((-1, 1))\n\n    # Copula Transformation (convert to ranks)\n    cx = np.column_stack([rankdata(xc, method='ordinal') for xc in x.T]) / float(x.size)\n    cy = np.column_stack([rankdata(yc, method='ordinal') for yc in y.T]) / float(y.size)\n\n    # Add bias term for affine projection\n    O = np.ones(cx.shape[0])\n    X = np.column_stack([cx, O])\n    Y = np.column_stack([cy, O])\n\n    # Random linear projections\n    Rx = (s / X.shape[1]) * np.random.randn(X.shape[1], k)\n    Ry = (s / Y.shape[1]) * np.random.randn(Y.shape[1], k)\n    X = np.dot(X, Rx)\n    Y = np.dot(Y, Ry)\n\n    # Apply non-linear function\n    fX = f(X)\n    fY = f(Y)\n\n    # Compute full covariance matrix\n    C = np.cov(np.hstack([fX, fY]).T)\n\n    # Find largest k with real-valued eigenvalues via binary search\n    k0 = k\n    lb = 1\n    ub = k\n    while True:\n        Cxx = C[:k, :k]\n        Cyy = C[k0:k0+k, k0:k0+k]\n        Cxy = C[:k, k0:k0+k]\n        Cyx = C[k0:k0+k, :k]\n\n        eigs = np.linalg.eigvals(\n            np.dot(np.dot(np.linalg.pinv(Cxx), Cxy),\n                   np.dot(np.linalg.pinv(Cyy), Cyx))\n        )\n\n        if not (np.all(np.isreal(eigs)) and 0 <= np.min(eigs) and np.max(eigs) <= 1):\n            ub -= 1\n            k = (ub + lb) // 2\n            continue\n        if lb == ub:\n            break\n        lb = k\n        if ub == lb + 1:\n            k = ub\n        else:\n            k = (ub + lb) // 2\n\n    return np.sqrt(np.max(eigs))\n\n\n# Import rankdata for RDC\nfrom scipy.stats import rankdata"
  },
  {
   "cell_type": "markdown",
   "id": "a7o3ukglk",
   "source": "### Mutual Information (Histogram-based Estimator)",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "212978ef",
   "metadata": {},
   "outputs": [],
   "source": "from sklearn.metrics import mutual_info_score\n\ndef calc_MI(x, y, bins=50):\n    \"\"\"\n    Estimate mutual information using histogram-based discretization.\n    \n    Parameters\n    ----------\n    x, y : array-like\n        Input samples\n    bins : int\n        Number of bins for histogram discretization\n    \n    Returns\n    -------\n    float\n        Estimated mutual information in nats\n    \"\"\"\n    # Create 2D histogram (contingency table)\n    c_xy = np.histogram2d(x, y, bins)[0]\n    # Compute MI from contingency table\n    mi = mutual_info_score(None, None, contingency=c_xy)\n    return mi"
  },
  {
   "cell_type": "markdown",
   "id": "lr6r60mqg4",
   "source": "### Distance Correlation\n\nDistance correlation is a measure of dependence between random vectors that is zero if and only if the vectors are independent.\n\nReference: Szekely, G. J., Rizzo, M. L., and Bakirov, N. K. (2007). \"Measuring and Testing Dependence by Correlation of Distances\".",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dc1b85a4",
   "metadata": {},
   "outputs": [],
   "source": "def dCor(x, y):\n    \"\"\"\n    Compute Distance Correlation between two samples.\n    \n    Distance correlation is zero if and only if the variables are independent.\n    \n    Reference:\n    Szekely, G. J., Rizzo, M. L., and Bakirov, N. K. (2007)\n    \"Measuring and Testing Dependence by Correlation of Distances\"\n    https://arxiv.org/pdf/0803.4101.pdf\n    \n    Parameters\n    ----------\n    x, y : array-like\n        Input samples\n    \n    Returns\n    -------\n    float\n        Distance correlation in [0, 1]\n    \"\"\"\n    return dcor.distance_correlation(x, y)"
  },
  {
   "cell_type": "markdown",
   "id": "id7nnihnkoh",
   "source": "### Maximal Information Coefficient (MIC)\n\nMIC is designed to capture a wide range of functional and non-functional relationships. It is based on mutual information but normalized to be in [0, 1].\n\nReference: Reshef et al. (2011). \"Detecting Novel Associations in Large Data Sets\".",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "61506920",
   "metadata": {},
   "outputs": [],
   "source": "def mic(x, y):\n    \"\"\"\n    Compute the Maximal Information Coefficient (MIC) approximation.\n    \n    Note: Using mutual information-based approximation since minepy \n    doesn't compile with Python 3.12.\n    \n    Parameters\n    ----------\n    x, y : array-like\n        Input samples\n    \n    Returns\n    -------\n    float\n        MIC approximation value in [0, 1]\n    \"\"\"\n    # Simple approximation using binned mutual information\n    bins = 10\n    x_binned = np.digitize(x, np.linspace(np.min(x), np.max(x), bins))\n    y_binned = np.digitize(y, np.linspace(np.min(y), np.max(y), bins))\n    \n    # Joint histogram\n    joint_hist, _, _ = np.histogram2d(x_binned, y_binned, bins=bins)\n    joint_hist = joint_hist / joint_hist.sum()\n    \n    # Marginals\n    px = joint_hist.sum(axis=1)\n    py = joint_hist.sum(axis=0)\n    \n    # Mutual information\n    mi = 0\n    for ii in range(bins):\n        for jj in range(bins):\n            if joint_hist[ii,jj] > 0 and px[ii] > 0 and py[jj] > 0:\n                mi += joint_hist[ii,jj] * np.log(joint_hist[ii,jj] / (px[ii] * py[jj]))\n    \n    # Normalize to [0, 1]\n    return min(1.0, mi / np.log(bins))"
  },
  {
   "cell_type": "markdown",
   "id": "w95zb85pc6",
   "source": "---\n\n## Comparing Dependence Measures\n\nWe will compare all four measures on different types of functional relationships:\n- **Linear:** $y = x$\n- **Quadratic:** $y = x^2$\n- **Sinusoidal:** $y = \\sin(x)$\n- **Logarithmic:** $y = \\log(x)$\n\n### Test Data Visualization",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e8ccf8d5",
   "metadata": {},
   "outputs": [],
   "source": "# Define test relationships\n# Linear: y = x\nx = np.linspace(0, 4, 50)\ny = x\nt1 = (x, y)\n\n# Quadratic: y = x^2\nx = np.linspace(-2, 2, 50)\ny = np.power(x, 2)\nt2 = (x, y)\n\n# Sinusoidal: y = sin(x)\nx = np.linspace(0, 2 * np.pi, 50)\ny = np.sin(x)\nt3 = (x, y)\n\n# Logarithmic: y = log(x)\nx = np.linspace(0.01, 10, 50)\ny = np.log(x)\nt4 = (x, y)\n\n# Visualize the test relationships\nplt.figure(figsize=(10, 8))\nplt.subplot(2, 2, 1)\nplt.plot(t1[0], t1[1], 'b-', linewidth=2)\nplt.title('Linear: $y = x$')\nplt.xlabel('x')\nplt.ylabel('y')\n\nplt.subplot(2, 2, 2)\nplt.plot(t2[0], t2[1], 'b-', linewidth=2)\nplt.title('Quadratic: $y = x^2$')\nplt.xlabel('x')\nplt.ylabel('y')\n\nplt.subplot(2, 2, 3)\nplt.plot(t3[0], t3[1], 'b-', linewidth=2)\nplt.title('Sinusoidal: $y = \\\\sin(x)$')\nplt.xlabel('x')\nplt.ylabel('y')\n\nplt.subplot(2, 2, 4)\nplt.plot(t4[0], t4[1], 'b-', linewidth=2)\nplt.title('Logarithmic: $y = \\\\log(x)$')\nplt.xlabel('x')\nplt.ylabel('y')\n\nplt.tight_layout()"
  },
  {
   "cell_type": "markdown",
   "id": "ofhmsor0mk",
   "source": "### Comparison Results\n\nNow let's compute each dependence measure for all four relationships and compare.",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "42e9440b",
   "metadata": {},
   "outputs": [],
   "source": "# Define test vectors and measure functions\ntest_vectors = [t1, t2, t3, t4]\ntest_vectors_str = ['linear', 'quadratic', 'sinusoidal', 'logarithmic']\ndependence_measures = [rdc, calc_MI, dCor, mic]\ndependence_measures_str = ['RDC', 'MI', 'dCor', 'MIC']\n\n# Compute all measures for all relationships\nresults = []\nfor ii, d in enumerate(dependence_measures):\n    dep_str = dependence_measures_str[ii]\n    for jj, t in enumerate(test_vectors):\n        test_str = test_vectors_str[jj]\n        x = t[0]\n        y = t[1]\n        r = d(x, y)\n        result_dict = {'Measure': dep_str, 'Relationship': test_str, 'Value': r}\n        results.append(result_dict)\n\n# Create DataFrame and plot\ndf = pd.DataFrame(results)\ng = sns.catplot(data=df, kind='bar', x='Relationship', y='Value', hue='Measure',\n                height=5, aspect=1.5)\ng.despine(left=True)\ng.set_axis_labels(\"Dependence Relationship\", \"Measure Value\")\ng.legend.set_title(\"Measure\")\nplt.title('Comparison of Dependence Measures')"
  },
  {
   "cell_type": "markdown",
   "id": "6totznppd1a",
   "source": "---\n\n## Robustness to Noise\n\nAn important property of dependence measures is how they degrade as noise increases. We test this by adding Gaussian noise to a linear relationship and observing how each measure responds.\n\n### Effect of Increasing Noise on Dependence Measures",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8cf46c8d",
   "metadata": {},
   "outputs": [],
   "source": "# Test robustness to noise on linear relationship\ntest = 'linear'\nkk = test_vectors_str.index(test)\ntest_fn = test_vectors[kk]\nxx = test_fn[0]\nyy = test_fn[1]\n\n# Range of noise standard deviations\nnoise_var = np.linspace(0, 4, 20)\nresults = np.zeros((len(dependence_measures), noise_var.shape[0]))\nnum_mc = 100  # Monte Carlo iterations for averaging\n\nfor ii, d in enumerate(dependence_measures):\n    for jj in range(noise_var.shape[0]):\n        std = np.sqrt(noise_var[jj])\n        mc_results = []\n        for kk in range(num_mc):\n            # Add Gaussian noise\n            y_noisy = yy + np.random.normal(scale=std, size=len(yy))\n            mc_results.append(d(xx, y_noisy))\n        results[ii, jj] = np.mean(mc_results)\n\n# Plot results\nfig, (ax1, ax2) = plt.subplots(2, 1, figsize=(10, 8))\n\n# Plot bounded measures (RDC, dCor, MIC)\nfor ii, dd in enumerate(dependence_measures_str):\n    if dd != 'MI':\n        ax1.plot(np.sqrt(noise_var), results[ii, :], 'o-', label=dd, linewidth=2)\nax1.legend()\nax1.set_title('Dependence Measures vs Noise Level (Linear Relationship)')\nax1.set_ylabel('Measure Value')\nax1.set_xlabel(r'Noise Standard Deviation ($\\sigma$)')\nax1.grid(True)\n\n# Plot MI separately (unbounded)\nfor ii, dd in enumerate(dependence_measures_str):\n    if dd == 'MI':\n        ax2.plot(np.sqrt(noise_var), results[ii, :], 'o-', label=dd, \n                 linewidth=2, color='green')\nax2.legend()\nax2.set_ylabel('Mutual Information (nats)')\nax2.set_xlabel(r'Noise Standard Deviation ($\\sigma$)')\nax2.grid(True)\n\nplt.tight_layout()"
  },
  {
   "cell_type": "markdown",
   "execution_count": null,
   "id": "3474ec38",
   "metadata": {},
   "outputs": [],
   "source": "---\n\n## Summary: Comparison of Dependence Measures\n\n| Measure | Range | Key Properties | Best Use Case |\n|---------|-------|----------------|---------------|\n| **Pearson Correlation** | $[-1, 1]$ | Fast, interpretable, but linear only | Linear relationships, Gaussian data |\n| **Mutual Information (MI)** | $[0, \\infty)$ | Information-theoretic, any dependence | When you need interpretable information content |\n| **Distance Correlation (dCor)** | $[0, 1]$ | Zero iff independent, any dependence | General dependence testing |\n| **MIC** | $[0, 1]$ | \"Equitability\" - similar scores for equal noise | Exploratory data analysis |\n| **RDC** | $[0, 1]$ | Fast approximation, uses copula transform | Large datasets, quick screening |\n\n### Key Observations\n\n1. **All alternative measures detect non-linear dependence** that Pearson correlation misses.\n\n2. **RDC, dCor, and MIC are bounded** in $[0, 1]$, making them easier to interpret.\n\n3. **MI is unbounded** but provides interpretable information content in nats/bits.\n\n4. **Performance varies with noise** - some measures are more robust than others.\n\n5. **Computational cost differs** - RDC is typically fastest, MIC can be slow for large datasets.\n\n---\n\n## Key Takeaways\n\n1. **Choose the right measure for your problem** - no single measure is universally best.\n\n2. **Non-linear dependence requires specialized measures** - Pearson correlation will miss it.\n\n3. **Consider computational constraints** - some measures scale poorly with data size.\n\n4. **These measures complement copulas** - they help identify dependence that copulas can model.\n\n---\n\n**Next:** With a foundation in dependence measures, we are now ready to introduce copulas and see how they provide a complete framework for modeling multivariate dependence."
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  },
  "latex_envs": {
   "LaTeX_envs_menu_present": true,
   "autoclose": false,
   "autocomplete": true,
   "bibliofile": "biblio.bib",
   "cite_by": "apalike",
   "current_citInitial": 1,
   "eqLabelWithNumbers": true,
   "eqNumInitial": 1,
   "hotkeys": {
    "equation": "Ctrl-E",
    "itemize": "Ctrl-I"
   },
   "labels_anchors": false,
   "latex_user_defs": false,
   "report_style_numbering": false,
   "user_envs_cfg": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}