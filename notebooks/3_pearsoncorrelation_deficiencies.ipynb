{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "k103q6kuq3c",
   "source": "# Deficiencies of the Pearson Correlation Coefficient\n\nWhile the Pearson correlation is widely used, it has significant limitations that can lead to misleading conclusions. In this notebook, we will explore:\n\n1. **Anscombe's Quartet** - Four datasets with identical correlations but very different relationships\n2. **Non-linear relationships** - Cases where correlation fails to capture dependence\n3. **The relationship between correlation and regression**\n\nUnderstanding these limitations is essential for knowing when to use more sophisticated dependence measures.\n\n---\n\n## Setup and Imports",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "efd97920",
   "metadata": {},
   "outputs": [],
   "source": "# Environment configuration for Jupyter\n%load_ext autoreload\n%autoreload 2\n%matplotlib notebook\n\n# Visualization libraries\nimport matplotlib.pyplot as plt\nfrom IPython import display\nimport seaborn as sns\n\n# Numerical computing\nimport time\nimport numpy as np\nfrom scipy.stats import multivariate_normal as mvn\n\n# Machine learning utilities\nfrom sklearn import linear_model\nimport sklearn.datasets as toy_datasets"
  },
  {
   "cell_type": "markdown",
   "id": "689724d6",
   "metadata": {},
   "source": "## Quick Review: Pearson Correlation\n\nRecall the population Pearson correlation coefficient:\n\n$$\\rho_{X,Y} = \\frac{\\text{Cov}(X,Y)}{\\sigma_X \\sigma_Y}$$\n\nwhere:\n- $\\text{Cov}(X,Y) = \\mathbb{E}[(X-\\mu_X)(Y-\\mu_Y)]$ is the covariance\n- $\\mu_X = \\mathbb{E}[X]$ is the mean of $X$\n- $\\mu_Y = \\mathbb{E}[Y]$ is the mean of $Y$\n- $\\sigma_X$ is the standard deviation of $X$\n- $\\sigma_Y$ is the standard deviation of $Y$"
  },
  {
   "cell_type": "markdown",
   "id": "537fbe72",
   "metadata": {},
   "source": "And the sample estimator:\n\n$$ r_{xy} = \\frac{\\sum_{i=1}^n (x_i - \\bar{x})(y_i - \\bar{y})}{\\sqrt{\\sum_{i=1}^n (x_i - \\bar{x})^2} \\sqrt{\\sum_{i=1}^n (y_i - \\bar{y})^2}} $$\n\n---\n\n## Anscombe's Quartet: Same Correlation, Different Data\n\nAnscombe's Quartet (1973) is a famous example demonstrating that datasets can have nearly identical statistical properties but look completely different. All four datasets have:\n- Same mean of $x$: 9\n- Same mean of $y$: 7.50\n- Same variance of $x$: 11\n- Same variance of $y$: 4.125\n- Same correlation: 0.816\n- Same regression line: $y = 3.00 + 0.500x$\n\nYet visually, they tell very different stories!"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9786d4f3",
   "metadata": {},
   "outputs": [],
   "source": "# Anscombe's Quartet data\nx = [10, 8, 13, 9, 11, 14, 6, 4, 12, 7, 5]\ny1 = [8.04, 6.95, 7.58, 8.81, 8.33, 9.96, 7.24, 4.26, 10.84, 4.82, 5.68]\ny2 = [9.14, 8.14, 8.74, 8.77, 9.26, 8.10, 6.13, 3.10, 9.13, 7.26, 4.74]\ny3 = [7.46, 6.77, 12.74, 7.11, 7.81, 8.84, 6.08, 5.39, 8.15, 6.42, 5.73]\nx4 = [8, 8, 8, 8, 8, 8, 8, 19, 8, 8, 8]\ny4 = [6.58, 5.76, 7.71, 8.84, 8.47, 7.04, 5.25, 12.50, 5.56, 7.91, 6.89]\n\ndatasets = {\n    'I': (x, y1),\n    'II': (x, y2),\n    'III': (x, y3),\n    'IV': (x4, y4)\n}\n\n# Create 2x2 subplot grid\nfig, axs = plt.subplots(2, 2, sharex=True, sharey=True, figsize=(8, 8),\n                        gridspec_kw={'wspace': 0.08, 'hspace': 0.08})\naxs[0, 0].set(xlim=(0, 20), ylim=(2, 14))\naxs[0, 0].set(xticks=(0, 10, 20), yticks=(4, 8, 12))\n\nfor ax, (label, (x_data, y_data)) in zip(axs.flat, datasets.items()):\n    # Add dataset label\n    ax.text(0.1, 0.9, label, fontsize=20, transform=ax.transAxes, va='top')\n    ax.tick_params(direction='in', top=True, right=True)\n    \n    # Plot data points\n    ax.plot(x_data, y_data, 'o')\n\n    # Fit and plot linear regression\n    p1, p0 = np.polyfit(x_data, y_data, deg=1)  # slope, intercept\n    ax.axline(xy1=(0, p0), slope=p1, color='r', lw=2)\n\n    # Add correlation coefficient in text box\n    stats = f'$r$ = {np.corrcoef(x_data, y_data)[0][1]:.2f}'\n    bbox = dict(boxstyle='round', fc='blanchedalmond', ec='orange', alpha=0.5)\n    ax.text(0.95, 0.07, stats, fontsize=9, bbox=bbox,\n            transform=ax.transAxes, horizontalalignment='right')\n\nplt.suptitle(\"Anscombe's Quartet: Same r, Different Relationships\", fontsize=14)\nplt.tight_layout()"
  },
  {
   "cell_type": "markdown",
   "id": "rmtwkixber",
   "source": "### Observations from Anscombe's Quartet\n\n- **Dataset I:** Linear relationship - correlation is appropriate\n- **Dataset II:** Quadratic relationship - correlation misses the structure\n- **Dataset III:** Perfect linear relationship with one outlier - outlier distorts correlation\n- **Dataset IV:** No relationship except one extreme point - single point drives the correlation\n\n**Lesson:** Always visualize your data! Correlation alone can be misleading.\n\n---\n\n## Non-Linear Relationships\n\nPearson correlation measures **linear** dependence only. For non-linear relationships, it can fail spectacularly.\n\n### Example 1: Quadratic Relationship ($y = x^2$)\n\nThis is a perfectly deterministic relationship, but the correlation is zero!",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ab77d768",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": "# Generate quadratic data\nx = np.linspace(-3, 3, 100)\ny = np.power(x, 2)\n\n# Create plot\nplt.figure(figsize=(8, 5))\n\n# Calculate correlation\nr = np.corrcoef(x, y)\nplt.title(r'$y = x^2$ : r = %.2f' % (r[0][1],))\nplt.grid(True)\n\n# Plot with regression line\ng = sns.regplot(x=x, y=y)\n\n# Add regression equation\nregr = linear_model.LinearRegression()\nregr.fit(x.reshape(-1, 1), y.reshape(-1, 1))\nprops = dict(boxstyle='round', alpha=0.5, color=sns.color_palette()[0])\ntextstr = 'y = %.2f + %.2fx' % (regr.intercept_[0], regr.coef_[0][0])\ng.text(0.1, 0.9, textstr, transform=g.transAxes, fontsize=14, bbox=props)\n\nplt.xlabel('x')\nplt.ylabel('y')"
  },
  {
   "cell_type": "markdown",
   "id": "g6fe81mgsl5",
   "source": "### Example 2: Logarithmic Relationship ($y = \\log(x)$)\n\nThe logarithmic relationship shows strong dependence but correlation may not fully capture it.",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a15a15ab",
   "metadata": {},
   "outputs": [],
   "source": "# Generate logarithmic data\nx = np.linspace(0.01, 3, 100)\ny = np.log(x)\n\n# Create plot\nplt.figure(figsize=(8, 5))\n\n# Calculate correlation\nr = np.corrcoef(x, y)\nplt.title(r'$y = \\log(x)$ : r = %.2f' % (r[0][1],))\nplt.grid(True)\n\n# Plot with regression line\ng = sns.regplot(x=x, y=y)\n\n# Add regression equation\nregr = linear_model.LinearRegression()\nregr.fit(x.reshape(-1, 1), y.reshape(-1, 1))\nprops = dict(boxstyle='round', alpha=0.5, color=sns.color_palette()[0])\ntextstr = 'y = %.2f + %.2fx' % (regr.intercept_[0], regr.coef_[0][0])\ng.text(0.1, 0.9, textstr, transform=g.transAxes, fontsize=14, bbox=props)\n\nplt.xlabel('x')\nplt.ylabel('y')"
  },
  {
   "cell_type": "markdown",
   "id": "ea1991f9",
   "metadata": {},
   "source": "---\n\n## Relationship Between Regression and Correlation\n\nThere is a direct mathematical relationship between the regression slope and the correlation coefficient.\n\nFor simple linear regression:\n$$Y_i = \\alpha + \\beta X_i + \\epsilon_i$$\n\nThe ordinary least squares (OLS) estimator of the slope is:\n$$\\hat{\\beta} = r_{xy} \\frac{s_y}{s_x}$$\n\nwhere:\n- $r_{xy}$ is the sample correlation coefficient\n- $s_x$ and $s_y$ are the sample standard deviations\n\n**Key Insight:** The correlation coefficient is essentially a standardized regression slope. When both variables have the same standard deviation, $r = \\hat{\\beta}$.\n\n---\n\n## Summary of Pearson Correlation Deficiencies\n\n| Deficiency | Example |\n|------------|---------|\n| Only measures **linear** dependence | $y = x^2$ has $r = 0$ despite perfect dependence |\n| Sensitive to **outliers** | A single extreme point can dominate the correlation |\n| Does not imply **causation** | High correlation does not mean one variable causes the other |\n| Not **invariant** to transformations | $\\rho(X, Y) \\neq \\rho(f(X), g(Y))$ for nonlinear $f, g$ |\n| Can be **zero** for dependent variables | Symmetric non-linear relationships yield $r = 0$ |\n\n---\n\n## Key Takeaways\n\n1. **Always visualize data** before relying on correlation.\n\n2. **Correlation measures linear dependence only** - it can completely miss non-linear relationships.\n\n3. **Outliers can dramatically affect correlation** - be cautious with extreme values.\n\n4. **We need better dependence measures** - especially for non-linear relationships and non-Gaussian data.\n\n---\n\n**Next:** We will explore alternative dependence measures that address these limitations, including Mutual Information, Distance Correlation, and the Maximal Information Coefficient."
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.13"
  },
  "latex_envs": {
   "LaTeX_envs_menu_present": true,
   "autoclose": false,
   "autocomplete": true,
   "bibliofile": "biblio.bib",
   "cite_by": "apalike",
   "current_citInitial": 1,
   "eqLabelWithNumbers": true,
   "eqNumInitial": 1,
   "hotkeys": {
    "equation": "Ctrl-E",
    "itemize": "Ctrl-I"
   },
   "labels_anchors": false,
   "latex_user_defs": false,
   "report_style_numbering": false,
   "user_envs_cfg": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}