{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b4f78551",
   "metadata": {},
   "outputs": [],
   "source": "# Setup\n%load_ext autoreload\n%autoreload 2\n%matplotlib notebook"
  },
  {
   "cell_type": "markdown",
   "id": "rcgo34832y",
   "source": "# Lesson 8: Fitting Copulas to Data\n\nThis notebook covers the practical aspects of fitting copulas to observed data. We will learn how to transform data to pseudo-observations, use rank-based dependence measures like Kendall's tau, and estimate copula parameters.\n\n## Learning Objectives\n- Understand the pseudo-observation transformation\n- Learn the relationship between Kendall's tau and copula parameters\n- Implement parameter estimation for common copula families",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "297538bc",
   "metadata": {},
   "outputs": [],
   "source": "# Import required libraries\nimport matplotlib.pyplot as plt\nfrom IPython import display\nimport seaborn as sns\nsns.set_theme(style=\"whitegrid\")\n\nimport numpy as np\nimport math\nfrom scipy.stats import norm, \\\n    beta, cauchy, expon, rayleigh, uniform, multivariate_t, t, \\\n    rankdata, kendalltau, pearsonr\nfrom scipy.stats.mstats import spearmanr\nimport scipy.integrate as integrate\nimport pandas as pd\nfrom scipy.optimize import brentq, fsolve"
  },
  {
   "cell_type": "markdown",
   "id": "ba23e58b",
   "metadata": {},
   "source": "## Pseudo-Observations\n\nTo fit a copula to data, we first need to transform the data to the unit cube. This is done using **pseudo-observations** (also called empirical copula data or probability integral transforms).\n\nThe idea is to replace each observation with its normalized rank. This removes the marginal distributions and leaves only the dependence structure.\n\n### Example: Original Data\n\n| x      | y |\n| ----------- | ----------- |\n| 0.6      | 0.8       |\n| 0.2   | 0.4        |\n| 1.2   | 0.5        |\n| 0.1   | 0.2        |"
  },
  {
   "cell_type": "markdown",
   "id": "f13dcccf",
   "metadata": {},
   "source": "### Rank Observations\n\nAfter ranking (smallest = 1, largest = n):\n\n| x      | y |\n| ----------- | ----------- |\n| 3   | 4    |\n| 2   | 2    |\n| 4   | 3    |\n| 1   | 1    |\n\nThe pseudo-observations are then $U_i = R_i / (n+1)$, where $R_i$ is the rank. We divide by $n+1$ instead of $n$ to avoid boundary issues at 0 and 1."
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fdb1cf1d",
   "metadata": {},
   "outputs": [],
   "source": "def pobs(X):\n    \"\"\"\n    Compute pseudo-observations from data.\n    \n    Pseudo-observations transform the data to the unit cube by replacing\n    each value with its normalized rank. This is essential for copula fitting\n    as it removes the marginal distributions.\n    \n    Parameters\n    ----------\n    X : ndarray of shape (n, d)\n        Input data with n samples and d dimensions\n    \n    Returns\n    -------\n    U : ndarray of shape (n, d)\n        Pseudo-observations in [0, 1]^d\n    \"\"\"\n    n, d = X.shape\n    # Divide by n+1 to avoid boundary issues (values exactly at 0 or 1)\n    U = rankdata(X, method='ordinal', axis=0) / float(n + 1)\n    return U"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1015c0e0",
   "metadata": {},
   "outputs": [],
   "source": "# Demonstrate pseudo-observations with Gaussian copula data\n# Reference: https://documentation.sas.com/doc/en/etsug/15.2/etsug_copula_details03.htm\n\n# Generate Gaussian copula samples\nr = 0.8  # Correlation parameter\nP = np.asarray([\n    [1, r],\n    [r, 1]\n])\nd = P.shape[0]\nn = 500\n\nA = np.linalg.cholesky(P)\nZ = np.random.normal(size=(n, d))\nU_Gauss = norm.cdf(np.matmul(Z, A))\n\n# Transform to Normal marginals\nH1 = np.empty_like(U_Gauss)\nH1[:, 0] = norm.ppf(U_Gauss[:, 0])\nH1[:, 1] = norm.ppf(U_Gauss[:, 1])\n\n# Transform to Beta and Cauchy marginals (different marginals, same copula)\nH2 = np.empty_like(U_Gauss)\nH2[:, 0] = beta.ppf(U_Gauss[:, 0], 2, 5)\nH2[:, 1] = cauchy.ppf(U_Gauss[:, 1])\n\n# Compute pseudo-observations for both cases\nU1 = pobs(H1)\nU2 = pobs(H2)"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d20ca230",
   "metadata": {},
   "outputs": [],
   "source": "# Visualize the data and their pseudo-observations\n# Key insight: Different marginals give different joint appearances,\n# but the pseudo-observations reveal the same underlying dependence structure\n\nplt.figure(figsize=(6, 6))\n\n# Original data with Normal marginals\nplt.subplot(2, 2, 1)\nplt.scatter(H1[:, 0], H1[:, 1], alpha=0.2)\ntau_result = kendalltau(H1[:, 0], H1[:, 1])\nplt.title(r'$\\rho=%0.02f$, $\\tau=%0.02f$' % (pearsonr(H1[:, 0], H1[:, 1])[0], tau_result.correlation))\nplt.xlabel('X (Normal)')\nplt.ylabel('Y (Normal)')\n\n# Original data with Beta and Cauchy marginals\nplt.subplot(2, 2, 2)\nplt.scatter(H2[:, 0], H2[:, 1], alpha=0.2)\ntau_result = kendalltau(H2[:, 0], H2[:, 1])\nplt.title(r'$\\rho=%0.02f$, $\\tau=%0.02f$' % (pearsonr(H2[:, 0], H2[:, 1])[0], tau_result.correlation))\nplt.xlabel('X (Beta)')\nplt.ylabel('Y (Cauchy)')\n\n# Pseudo-observations from Normal marginals\nplt.subplot(2, 2, 3)\nplt.scatter(U1[:, 0], U1[:, 1], alpha=0.2)\nplt.xlabel('U')\nplt.ylabel('V')\nplt.title('Pseudo-observations')\n\n# Pseudo-observations from Beta/Cauchy marginals\nplt.subplot(2, 2, 4)\nplt.scatter(U2[:, 0], U2[:, 1], alpha=0.2)\nplt.xlabel('U')\nplt.ylabel('V')\nplt.title('Pseudo-observations')\n\nplt.tight_layout()"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "17cd4631",
   "metadata": {},
   "outputs": [],
   "source": "# Demonstrate pseudo-observations with Clayton copula data\n# Reference: https://medium.com/@financialnoob/introduction-to-copulas-part-2-9de74010ed87\n\nalpha = 6  # Clayton parameter\nu_rand = np.random.rand(n)\nt_rand = np.random.rand(n)\nv_rand = ((t_rand / u_rand**(-alpha-1))**(-alpha/(1+alpha)) - u_rand**(-alpha) + 1)**(-1/alpha)\nU_clayton = np.vstack([u_rand, v_rand]).T\n\n# Transform to Beta and Cauchy marginals\nH2 = np.empty_like(U_clayton)\nH2[:, 0] = beta.ppf(U_clayton[:, 0], 2, 5)\nH2[:, 1] = cauchy.ppf(U_clayton[:, 1])\n\nplt.figure(figsize=(6, 3))\n\n# Joint distribution with Beta/Cauchy marginals\nplt.subplot(1, 2, 1)\nplt.scatter(H2[:, 0], H2[:, 1], alpha=0.2)\nplt.xlabel('$\\\\mathrm{Beta}(2,5)$')\nplt.ylabel('$\\\\mathrm{Cauchy}(0,1)$')\nplt.title('Original Data')\n\n# Pseudo-observations reveal the Clayton copula structure\nU = pobs(H2)\nplt.subplot(1, 2, 2)\nplt.scatter(U[:, 0], U[:, 1], alpha=0.2)\nplt.xlabel('U')\nplt.ylabel('V')\nplt.title('Pseudo-observations')\n\nplt.tight_layout()"
  },
  {
   "cell_type": "markdown",
   "id": "e79470e5",
   "metadata": {},
   "source": "## Parameter Estimation Using Kendall's Tau\n\nKendall's tau ($\\tau$) is a rank-based measure of dependence that is invariant to monotonic transformations. This makes it ideal for copula parameter estimation.\n\n### Kendall's Tau\n\n**Theoretical definition** (in terms of copula):\n$$ \\tau = 4 \\int_0^1 \\int_0^1 C(u,v) \\, dC(u,v) - 1 $$\n\n**Sample estimator**:\n$$ \\hat{\\tau} = \\frac{2}{n(n-1)} \\sum_{i<j} \\text{sgn}(x_i-x_j) \\cdot \\text{sgn}(y_i-y_j) $$\n\nThe key advantage: Kendall's tau has simple closed-form relationships with copula parameters for many common families."
  },
  {
   "cell_type": "markdown",
   "id": "5c4f44cf",
   "metadata": {},
   "source": "### Gaussian Copula Parameter Estimation\n\nFor the Gaussian copula with correlation parameter $\\rho$:\n$$ \\rho = \\sin\\left(\\tau \\cdot \\frac{\\pi}{2}\\right) $$\n\nFor Spearman's rho:\n$$ \\rho = 2\\sin\\left(\\rho_S \\cdot \\frac{\\pi}{6}\\right) $$"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "73115a63",
   "metadata": {},
   "outputs": [],
   "source": "def ktau2gaussian(val, dependency='kendall'):\n    \"\"\"\n    Convert Kendall's tau or Spearman's rho to Gaussian copula correlation parameter.\n    \n    Parameters\n    ----------\n    val : float\n        Dependence measure value\n    dependency : str\n        Type of measure: 'kendall' or 'spearman'\n    \n    Returns\n    -------\n    rho : float\n        Gaussian copula correlation parameter\n    \"\"\"\n    if dependency == 'kendall':\n        rho = np.sin(val * math.pi / 2.0)\n    elif dependency == 'spearman':\n        rho = 2 * np.sin(val * math.pi / 6.0)\n    return rho\n\n# Alternative: Maximum Likelihood approach (commented pseudo-code)\n# fun = @(theta) -sum(log(copulapdf(family,u,theta)+eps));\n# theta = fminbnd(fun,-1,1,optimset('Display','off'));"
  },
  {
   "cell_type": "markdown",
   "id": "08911b28",
   "metadata": {},
   "source": "### Clayton Copula Parameter Estimation\n\nFor the Clayton copula with parameter $\\alpha > 0$:\n$$ \\alpha = \\frac{2\\tau}{1-\\tau} $$\n\nNote: This relationship requires $\\tau \\in [0, 1)$ (positive dependence only)."
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "17245a41",
   "metadata": {},
   "outputs": [],
   "source": "def ktau2clayton(val, dependency='kendall'):\n    \"\"\"\n    Convert Kendall's tau to Clayton copula parameter.\n    \n    Parameters\n    ----------\n    val : float\n        Kendall's tau value (must be in [0, 1))\n    dependency : str\n        Type of measure (only 'kendall' supported)\n    \n    Returns\n    -------\n    alpha : float\n        Clayton copula parameter\n    \"\"\"\n    if dependency == 'kendall':\n        if val < 0 or val >= 1:\n            raise ValueError(\"Valid values of Kendall's Tau for Clayton Copula are [0, 1)\")\n        alpha = 2.0 * val / (1.0 - val)\n    elif dependency == 'spearman':\n        raise NotImplementedError(\"Spearman's rho currently unsupported for Clayton Copula family!\")\n    \n    return alpha"
  },
  {
   "cell_type": "markdown",
   "id": "df5cd07d",
   "metadata": {},
   "source": "### Gumbel Copula Parameter Estimation\n\nFor the Gumbel copula with parameter $\\alpha \\geq 1$:\n$$ \\alpha = \\frac{1}{1-\\tau} $$\n\nNote: This relationship requires $\\tau \\in [0, 1)$ (positive dependence only)."
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4c64dd88",
   "metadata": {},
   "outputs": [],
   "source": "def ktau2gumbel(val, dependency='kendall'):\n    \"\"\"\n    Convert Kendall's tau to Gumbel copula parameter.\n    \n    Parameters\n    ----------\n    val : float\n        Kendall's tau value (must be in [0, 1))\n    dependency : str\n        Type of measure (only 'kendall' supported)\n    \n    Returns\n    -------\n    alpha : float\n        Gumbel copula parameter\n    \"\"\"\n    if dependency == 'kendall':\n        if val < 0 or val >= 1:\n            raise ValueError(\"Valid values of Kendall's Tau for Gumbel Copula are [0, 1)\")\n        alpha = 1.0 / (1.0 - val)\n    elif dependency == 'spearman':\n        raise NotImplementedError(\"Spearman's rho currently unsupported for Gumbel Copula family!\")\n    \n    return alpha"
  },
  {
   "cell_type": "markdown",
   "id": "267ec999",
   "metadata": {},
   "source": "### Frank Copula Parameter Estimation\n\nFor the Frank copula, the relationship involves the Debye function $\\mathcal{D}_n(x)$:\n\n$$ \\tau = 1 - \\frac{4}{\\alpha}\\left(1 - \\mathcal{D}_1(\\alpha)\\right) $$\n\nwhere the Debye function is:\n$$ \\mathcal{D}_n(x) = \\frac{n}{x^n} \\int_0^x \\frac{t^n}{e^t-1} dt $$\n\nSince there is no closed-form inverse, we use numerical root finding."
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6d846f69",
   "metadata": {},
   "outputs": [],
   "source": "def debye(x, n):\n    \"\"\"\n    Evaluate the Debye function.\n    \n    The Debye function appears in the relationship between Kendall's tau\n    and the Frank copula parameter.\n    \n    See: http://en.wikipedia.org/wiki/Debye_function\n    \n    Parameters\n    ----------\n    x : float\n        Input value\n    n : int\n        Order of the Debye function\n    \n    Returns\n    -------\n    float\n        D_n(x)\n    \"\"\"\n    n = float(n)  # Ensure n is a float for division\n    sol = integrate.quad(lambda t: pow(t, n) / (np.exp(t) - 1.0), 0.0, x)\n    return n * sol[0] / pow(x, n)\n\n\ndef _frank_kendall_fopt(alpha, tau):\n    \"\"\"Objective function for Frank copula parameter estimation.\"\"\"\n    return 4 * (debye(alpha, 1) - 1) / alpha + 1 - tau\n\n\ndef ktau2frank(val, dependency='kendall'):\n    \"\"\"\n    Convert Kendall's tau to Frank copula parameter.\n    \n    Uses numerical root finding since there is no closed-form solution.\n    \n    Parameters\n    ----------\n    val : float\n        Kendall's tau value\n    dependency : str\n        Type of measure (only 'kendall' supported)\n    \n    Returns\n    -------\n    alpha : float\n        Frank copula parameter\n    \"\"\"\n    if dependency == 'kendall':\n        return fsolve(_frank_kendall_fopt, 1, args=(val))[0]\n    elif dependency == 'spearman':\n        raise NotImplementedError(\"Spearman's rho currently unsupported for Frank Copula family!\")\n    \n    return None"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fd14f8ec",
   "metadata": {},
   "outputs": [],
   "source": "## Validation Examples\n\nLet us validate our parameter estimation methods by generating data from known copulas\nand checking if we can recover the true parameters.\n\n### Gaussian Copula Validation\n\n# True parameter\nr_true = 0.5\n\n# Generate Gaussian copula samples\nP = np.asarray([\n    [1, r_true],\n    [r_true, 1]\n])\nd = P.shape[0]\nn = 1000\n\nA = np.linalg.cholesky(P)\nZ = np.random.normal(size=(n, d))\nU_Gauss = norm.cdf(np.matmul(Z, A))\n\n# Transform to different marginals\nH1 = np.empty_like(U_Gauss)\nH1[:, 0] = norm.ppf(U_Gauss[:, 0])\nH1[:, 1] = norm.ppf(U_Gauss[:, 1])\n\nH2 = np.empty_like(U_Gauss)\nH2[:, 0] = beta.ppf(U_Gauss[:, 0], 2, 5)\nH2[:, 1] = cauchy.ppf(U_Gauss[:, 1])\n\n# Estimate parameters using Kendall's tau\ntau1 = kendalltau(H1[:, 0], H1[:, 1])\ntau2 = kendalltau(H2[:, 0], H2[:, 1])\nrho_est1 = ktau2gaussian(tau1.correlation)\nrho_est2 = ktau2gaussian(tau2.correlation)\n\nprint(\"Gaussian Copula Parameter Recovery\")\nprint(f\"True rho: {r_true}\")\nprint(f\"Estimated rho (Normal marginals): {rho_est1:.4f}\")\nprint(f\"Estimated rho (Beta/Cauchy marginals): {rho_est2:.4f}\")\nprint(f\"Pearson correlation (Normal): {pearsonr(H1[:, 0], H1[:, 1])[0]:.4f}\")\nprint(f\"Pearson correlation (Beta/Cauchy): {pearsonr(H2[:, 0], H2[:, 1])[0]:.4f}\")"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "36eeb910",
   "metadata": {},
   "outputs": [],
   "source": "### Clayton Copula Validation\n\n# True parameter\nalpha_true = 6\n\n# Generate Clayton copula samples\nu_rand = np.random.rand(n)\nt_rand = np.random.rand(n)\nv_rand = ((t_rand / u_rand**(-alpha_true-1))**(-alpha_true/(1+alpha_true)) - u_rand**(-alpha_true) + 1)**(-1/alpha_true)\nU_clayton = np.vstack([u_rand, v_rand]).T\n\n# Transform to different marginals\nH1 = np.empty_like(U_clayton)\nH1[:, 0] = norm.ppf(U_clayton[:, 0])\nH1[:, 1] = norm.ppf(U_clayton[:, 1])\n\nH2 = np.empty_like(U_clayton)\nH2[:, 0] = beta.ppf(U_clayton[:, 0], 2, 5)\nH2[:, 1] = cauchy.ppf(U_clayton[:, 1])\n\n# Estimate parameters using Kendall's tau\ntau1 = kendalltau(H1[:, 0], H1[:, 1])\ntau2 = kendalltau(H2[:, 0], H2[:, 1])\nalpha_est1 = ktau2clayton(tau1.correlation)\nalpha_est2 = ktau2clayton(tau2.correlation)\n\nprint(\"\\nClayton Copula Parameter Recovery\")\nprint(f\"True alpha: {alpha_true}\")\nprint(f\"Estimated alpha (Normal marginals): {alpha_est1:.4f}\")\nprint(f\"Estimated alpha (Beta/Cauchy marginals): {alpha_est2:.4f}\")"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fd1ff611",
   "metadata": {},
   "outputs": [],
   "source": "### Gumbel Copula Validation\n\n# Gumbel copula generator functions\ndef gumbel_phi(t, alpha):\n    \"\"\"Gumbel generator function\"\"\"\n    return (-np.log(t))**alpha\n\ndef gumbel_phi_inv(t, alpha):\n    \"\"\"Inverse of Gumbel generator function\"\"\"\n    return np.exp(-t**(1/alpha))\n\ndef gumbel_K(t, alpha):\n    \"\"\"Kendall distribution function for Gumbel copula\"\"\"\n    return t * (alpha - np.log(t)) / alpha\n\n# True parameter\nalpha_true = 6\n\n# Generate Gumbel copula samples\nt1_rand = np.random.rand(n)\nt2_rand = np.random.rand(n)\n\nw = []\nfor t_val in t2_rand:\n    func = lambda w: gumbel_K(w, alpha=alpha_true) - t_val\n    w.append(brentq(func, 0.0000000001, 0.9999999999))\nw = np.array(w).flatten()\n\nu = gumbel_phi_inv(t1_rand * gumbel_phi(w, alpha=alpha_true), alpha=alpha_true)\nv = gumbel_phi_inv((1-t1_rand) * gumbel_phi(w, alpha=alpha_true), alpha=alpha_true)\nU_gumbel = np.vstack([u, v]).T\n\n# Transform to different marginals\nH1 = np.empty_like(U_gumbel)\nH1[:, 0] = norm.ppf(U_gumbel[:, 0])\nH1[:, 1] = norm.ppf(U_gumbel[:, 1])\n\nH2 = np.empty_like(U_gumbel)\nH2[:, 0] = beta.ppf(U_gumbel[:, 0], 2, 5)\nH2[:, 1] = cauchy.ppf(U_gumbel[:, 1])\n\n# Estimate parameters using Kendall's tau\ntau1 = kendalltau(H1[:, 0], H1[:, 1])\ntau2 = kendalltau(H2[:, 0], H2[:, 1])\nalpha_est1 = ktau2gumbel(tau1.correlation)\nalpha_est2 = ktau2gumbel(tau2.correlation)\n\nprint(\"\\nGumbel Copula Parameter Recovery\")\nprint(f\"True alpha: {alpha_true}\")\nprint(f\"Estimated alpha (Normal marginals): {alpha_est1:.4f}\")\nprint(f\"Estimated alpha (Beta/Cauchy marginals): {alpha_est2:.4f}\")"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8ff38671",
   "metadata": {},
   "outputs": [],
   "source": "### Frank Copula Validation\n\n# Frank copula generator functions\ndef frank_phi(t, alpha):\n    \"\"\"Frank generator function\"\"\"\n    return -np.log((np.exp(-alpha*t) - 1) / (np.exp(-alpha) - 1))\n\ndef frank_phi_inv(t, alpha):\n    \"\"\"Inverse of Frank generator function\"\"\"\n    return -1/alpha * np.log((np.exp(-alpha) - 1) / np.exp(t) + 1)\n\ndef frank_K(t, alpha):\n    \"\"\"Kendall distribution function for Frank copula\"\"\"\n    return (t + (1 - np.exp(alpha*t)) * np.log((1-np.exp(alpha*t)) * \n                                               np.exp(-alpha*t+alpha) / (1-np.exp(alpha))) / alpha)\n\n# True parameter\nalpha_true = 6\n\n# Generate Frank copula samples\nt1_rand = np.random.rand(n)\nt2_rand = np.random.rand(n)\n\nw = []\nfor t_val in t2_rand:\n    func = lambda w: frank_K(w, alpha=alpha_true) - t_val\n    w.append(brentq(func, 0.0000000001, 0.9999999999))\nw = np.array(w).flatten()\n\nu = frank_phi_inv(t1_rand * frank_phi(w, alpha=alpha_true), alpha=alpha_true)\nv = frank_phi_inv((1-t1_rand) * frank_phi(w, alpha=alpha_true), alpha=alpha_true)\nU_frank = np.vstack([u, v]).T\n\n# Transform to different marginals\nH1 = np.empty_like(U_frank)\nH1[:, 0] = norm.ppf(U_frank[:, 0])\nH1[:, 1] = norm.ppf(U_frank[:, 1])\n\nH2 = np.empty_like(U_frank)\nH2[:, 0] = beta.ppf(U_frank[:, 0], 2, 5)\nH2[:, 1] = cauchy.ppf(U_frank[:, 1])\n\n# Estimate parameters using Kendall's tau\ntau1 = kendalltau(H1[:, 0], H1[:, 1])\ntau2 = kendalltau(H2[:, 0], H2[:, 1])\nalpha_est1 = ktau2frank(tau1.correlation)\nalpha_est2 = ktau2frank(tau2.correlation)\n\nprint(\"\\nFrank Copula Parameter Recovery\")\nprint(f\"True alpha: {alpha_true}\")\nprint(f\"Estimated alpha (Normal marginals): {alpha_est1:.4f}\")\nprint(f\"Estimated alpha (Beta/Cauchy marginals): {alpha_est2:.4f}\")"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eef31b93",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}